[
    {
        "model_type": "openai_chat",
        "config_name": "gpt-4",
        "model_name": "gpt-4",
        "api_key": "xxx",
        "organization": "xxx",
        "generate_args": {
            "temperature": 0.5
        }
    },
    {
        "model_type": "post_api_chat",
        "config_name": "my_post_api",
        "api_url": "https://xxx",
        "headers": {},
        "json_args": {}
    },
    {
        "model_type": "post_api_chat",
        "config_name": "llama3",
        "api_url": "http://127.0.0.1:8000/llm/",
        "json_args": {
          "max_length": 3584,
          "temperature": 0.5
        }
    },
    {
        "model_type": "post_api_embedding",
        "config_name": "qwen2_emb_config",
        "api_url": "http://127.0.0.1:8000/embedding/",
        "json_args": {
          "max_length": 3584,
          "temperature": 0.5
        }
    },
    {
        "config_name": "llama3_emb_config",
        "model_type": "ollama_embedding",
        "model_name": "llama3",
        "options": {
            "temperature": 0.5
        },
        "keep_alive": "5m"
    },
    {
        "config_name": "ollama_embedding-qwen:0.5b",
        "model_type": "ollama_embedding",
        "model_name": "qwen:0.5b",
        "options": {
            "temperature": 0.7
        },
        "keep_alive": "5m"
    },    
    {
        "config_name": "ollama_llama3_8b",
        "model_type": "ollama_chat",
        "model_name": "llama3",
        "options": {
            "temperature": 0.5,
            "seed": 123
        },
        "keep_alive": "5m"
     },
     {
        "config_name": "ollama_llama3_70b",
        "model_type": "ollama_chat",
        "model_name": "llama3:70b",
        "options": {
            "temperature": 0.5,
            "seed": 123
        },
        "keep_alive": "5m"
     }
]